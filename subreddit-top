#!/usr/bin/env ruby
# frozen_string_literal: true

require 'cgi'
require 'json'
require 'net/http'
require 'nokogiri'
require 'uri'
require 'pathname'

########## EDIT ##########
# Configure feed info
Site_url = URI.parse('https://reddit.com/')
Feed_title = 'Subreddit top post amalgamation'

########## LEAVE ##########
# Check for common required environment variables
begin
  Feed_local_dir = Pathname.new(ENV['FEED_LOCAL_DIR'])
  Feed_web_dir = Pathname.new(ENV['FEED_WEB_DIR'])
  Feed_settings_dir = Pathname.new(ENV['FEED_SETTINGS_DIR'])

  raise Errno::ENOENT unless Feed_local_dir.directory? && Feed_web_dir && Feed_settings_dir.directory?
rescue TypeError, Errno::ENOENT
  abort <<~ERROR
    Some required environment variables are not set correctly.
    Full list to verify:

    FEED_LOCAL_DIR: path to directory to read and write feed
    FEED_WEB_DIR: url to web directory hosting the feed
    FEED_SETTINGS_DIR: path to directory containing feed script config files
  ERROR
end

# Prepare the feed
Feed_slug = Feed_title.downcase.strip.gsub(' ', '-').gsub(/[^\w-]/, '')
Feed_file = Feed_local_dir.join("#{Feed_slug}.json")
Feed_location = Feed_web_dir.join(Feed_file.basename)

Previous_items = begin
  JSON.parse(Feed_file.read)['items']
rescue Errno::ENOENT
  [{}]
end

Previous_newest_id = Previous_items.first['id']

########## EDIT ##########
# Strategy for getting new items
Image_extensions = ['.gif', '.jpg', '.png'].freeze
User_agent = 'Listing top posts on a private RSS feed on linux:subreddittop:0.0.1 (no account)'

Config_file = Feed_settings_dir.join('subreddits.txt')
abort "Missing #{Config_file}" unless Config_file.file?

# Config file should be of the form (one per line):
# subreddit_name | check_period
# Example:
# wallpapers | day
# surrealism | month
# check period should be one ofs#{check_periods} and defaults to #{default_period}

Subreddits = lambda {
  check_periods = %w[day week month year all].freeze
  default_period = 'week'

  lines = Config_file.readlines(chomp: true)

  lines.each_with_object({}) { |line, hash|
    subreddit, check_in = line.split('|').map(&:strip)
    period = check_periods.include?(check_in) ? check_in : default_period

    hash[subreddit] = period
  }
}.call

fetch_reddit_json = lambda { |user_agent, url|
  begin
    tries ||= 3

    response = Net::HTTP.get_response(url, 'User-Agent' => user_agent)

    case response
    when Net::HTTPSuccess
      JSON.parse(response.body)
    when Net::HTTPRedirection
      location = URI.parse(response['location'])
      fetch_reddit_json(location)
    end
  rescue Net::OpenTimeout
    unless (tries -= 1).zero?
      puts 'Encountered a timeout. Retrying in a few seconds.'
      sleep 10
      retry
    end

    abort 'Failed to get JSON.'
  end
}.curry.call(User_agent)

New_items = Subreddits.each_with_object([]) { |(subreddit, period), array|
  warn "Checking #{subreddit}â€¦"
  sleep 3 # Avoid rate limiting

  sub_url = URI.parse("https://www.reddit.com/r/#{subreddit}/top.json?t=#{period}&limit=1")

  top_item = fetch_reddit_json.call(sub_url)['data']['children'].first['data']
  item_url = URI.join(Site_url, top_item['permalink'].split('/').map { |a| CGI.escape(a) }.join('/'))
  item_title = top_item['title']

  content = lambda {
    top_post_json_url = URI.parse("#{item_url}.json")
    data = fetch_reddit_json.call(top_post_json_url).first['data']['children'].first['data']

    html = []
    html.push("<p>From r/#{subreddit}</p>")

    # If post is just text, return it now
    text = Nokogiri::HTML.parse(data['selftext_html'])
    html.push(text) unless text.nil?

    return html if data['url'] == item_url

    # If post is video, embed it
    video_preview = data.dig('secure_media', 'reddit_video', 'fallback_url') ||
                    data.dig('preview', 'reddit_video_preview', 'fallback_url')

    if video_preview
      html.push("<video><source src='#{video_preview}'></video>")
      return html
    end

    # If post is image, embed it
    if Image_extensions.any? { |ext| Pathname.new(data['url']).extname == ext }
      html.push("<img src='#{data['url']}'>")
      return html
    end

    # If post is gallery, embed all images
    if data['is_gallery']
      data['media_metadata'].each do |metadata|
        html.push("<img src='#{metadata[1]['s']['u'].gsub('&amp;', '&')}'>")
      end
    end

    # If post is a URL
    html.push("<a href='#{data['url']}'>#{data['url']}</a>")

    html
  }.call.join

  array.push(title: item_title, id: item_url, url: "#{item_url}.compact", content_html: content)
}

########## LEAVE ##########
# If there are no new items, inform and exit
if New_items.empty?
  puts 'No new items to add'
  exit 0
end

# Rebuild feed
Feed = {
  version: 'https://jsonfeed.org/version/1',
  title: Feed_title,
  home_page_url: Site_url.to_s,
  feed_url: Feed_location.to_path,

  # Prepend new items to the old and limit the amount of items
  items: (New_items + Previous_items).first(100)
}

Feed_file.write(JSON.pretty_generate(Feed))
