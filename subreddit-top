#!/usr/bin/env ruby
# frozen_string_literal: true

require 'cgi'
require 'json'
require 'net/http'
require 'nokogiri'
require 'uri'
require 'pathname'

########## EDIT ##########
# Configure feed info
Site_url = URI.parse('https://reddit.com/')
Feed_title = 'Subreddit top post amalgamation'

########## LEAVE ##########
# Check for common required environment variables
begin
  Feed_local_dir = Pathname.new(ENV['FEED_LOCAL_DIR'])
  Feed_web_dir = Pathname.new(ENV['FEED_WEB_DIR'])
  Feed_settings_dir = Pathname.new(ENV['FEED_SETTINGS_DIR'])

  raise Errno::ENOENT unless Feed_local_dir.directory? && Feed_web_dir && Feed_settings_dir.directory?
rescue TypeError, Errno::ENOENT
  abort <<~ERROR
    Some required environment variables are not set correctly.
    Full list to verify:

    FEED_LOCAL_DIR: path to directory to read and write feed
    FEED_WEB_DIR: url to web directory hosting the feed
    FEED_SETTINGS_DIR: path to directory containing feed script config files
  ERROR
end

# Prepare the feed
Feed_slug = Feed_title.downcase.strip.gsub(' ', '-').gsub(/[^\w-]/, '')
Feed_file = Feed_local_dir.join("#{Feed_slug}.json")
Feed_location = Feed_web_dir.join(Feed_file.basename)

Previous_items = begin
  JSON.parse(Feed_file.read)['items']
rescue Errno::ENOENT
  [{}]
end

Previous_newest_id = Previous_items.first['id']

########## EDIT ##########
# Strategy for getting new items
Config_file = Feed_settings_dir.join('subreddits.txt')
abort "Missing #{Config_file}" unless Config_file.file?

Check_period = 'week'
Subreddits = Config_file.readlines.map(&:strip)
Image_extensions = ['.gif', '.jpg', '.png']
User_agent = 'Ruby script on linux:subreddittop:0.0.1 (no account)'

def fetch_reddit_json(url)
  response = Net::HTTP.get_response(url, 'User-Agent' => User_agent)

  case response
  when Net::HTTPSuccess
    JSON.parse(response.body)
  when Net::HTTPRedirection
    location = URI.parse(response['location'])
    fetch_reddit_json(location)
  end
end

New_items = Subreddits.each_with_object([]) do |subreddit, array|
  warn "Checking #{subreddit}â€¦"
  sleep 3 # Avoid rate limiting

  sub_url = URI.parse("https://www.reddit.com/r/#{subreddit}/top.json?t=#{Check_period}&limit=1")

  top_item = fetch_reddit_json(sub_url)['data']['children'].first['data']
  item_url = URI.join(Site_url, top_item['permalink'].split('/').map { |a| CGI.escape(a) }.join('/'))
  item_title = top_item['title']

  top_post = lambda {
    json_url = URI.parse("#{item_url}.json")
    data = fetch_reddit_json(json_url).first['data']['children'].first['data']

    {
      text: Nokogiri::HTML.parse(data['selftext_html']),
      url: data['url'],
      video_preview: data.dig('secure_media', 'reddit_video', 'fallback_url') || data.dig('preview', 'reddit_video_preview', 'fallback_url')
    }
  }.call

  content = lambda {
    html = []
    html.push("<p>From r/#{subreddit}</p>")
    html.push(top_post[:text]) unless top_post[:text].nil?

    # If post is just text, return it now
    return html if top_post[:url] == item_url

    # If post is video, embed it
    if top_post[:video_preview]
      html.push("<video><source src='#{top_post[:video_preview]}'></video>")
      return html
    end

    # If post is image, embed it
    if Image_extensions.any? { |ext| Pathname(top_post[:url]).extname == ext }
      html.push("<img src='#{top_post[:url]}'>")
      return html
    end

    # If post is a URL
    html.push("<a href='#{top_post[:url]}'>#{top_post[:url]}</a>")

    html
  }.call.join

  array.push(title: item_title, id: item_url, url: item_url, content_html: content)
end

########## LEAVE ##########
# If there are no new items, inform and exit
if New_items.empty?
  puts 'No new items to add'
  exit 0
end

# Rebuild feed
Feed = {
  version: 'https://jsonfeed.org/version/1',
  title: Feed_title,
  home_page_url: Site_url.to_s,
  feed_url: Feed_location.to_path,

  # Prepend new items to the old and limit the amount of items
  items: (New_items + Previous_items).first(100)
}

Feed_file.write(JSON.pretty_generate(Feed))
